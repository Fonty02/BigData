\section{Modelling}

This section details the deep learning architectures employed for molecular property prediction and introduces the proposed adaptive fine tuning strategy designed to optimize the trade-off between performance and environmental impact.

\subsection{Architectures}

Molecular representation learning is approached through two distinct paradigms: sequence-based modelling (treating molecules as SMILES strings) and graph-based modelling (treating molecules as molecular graphs).

\subsubsection{Sequence-based Models (Transformers)}

In this paradigm, molecules are represented as SMILES strings.
The Transformer architecture \cite{Attention} is leveraged, specifically models pre-trained on large chemical corpora via Masked Language Modeling (MLM).
The specific models evaluated are:
\begin{itemize}
	\item \textbf{ChemBERTa} 
    \item \textbf{ChemBERTa-2}
	\item \textbf{SELFormer} 
    \item \textbf{SMILES-BERT}
\end{itemize}

\subsubsection{Graph Neural Networks}

Alternatively, molecules are naturally represented as graphs $G=(V, E)$, where atoms constitute the nodes $V$ and chemical bonds the edges $E$.
To this end, \textbf{GraphMAE} is employed. Unlike standard GNNs, GraphMAE focuses on self-supervised learning by masking a portion of the input graph (nodes or edges) and reconstructing it, forcing the model to learn robust structural representations.

\subsection{GFT Strategy}

To address the environmental concerns of fine tuning deep learning models, a custom callback mechanism is introduced: the Green FineTuning.
Standard fine tuning procedures typically rely on "patience" based solely on validation loss.
In contrast, this approach integrates energy consumption directly into the stopping criterion.

\subsubsection{Formal Definition}

The core idea is to interrupt fine tuning when the marginal gain in predictive performance no longer justifies the marginal energy cost.
We define the instantaneous GFT ratio ($GFT_t$) at epoch $t$ as:
\begin{equation}
	GFT_t = \frac{\Delta P_t}{\Delta E_t} = \frac{Perf_t - Perf_{t-1}}{Emission_t - Emission_{t-1}}
\end{equation}

where $\Delta P_t$ represents the percentage improvement in the validation metric and $\Delta E_t$ is the incremental CO$_2$eq produced during the epoch.

\subsubsection{Stopping Condition}

Since raw metrics can be volatile, we employ an Exponential Moving Average (EMA) to smooth the GFT signal, ensuring stability in the decision process.
The smoothed value $S_t$ is updated as follows:
\begin{equation}
	S_t = \alpha \cdot GFT_t + (1 - \alpha) \cdot S_{t-1}
\end{equation}

where $\alpha \in [0, 1]$ is the smoothing factor.

\subsection{Fine tuning details and hyperparameters}

\begin{table}[h]
    \centering
    \scriptsize
    \begin{tabular}{c|c|c}
        \hline
        \textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
        \hline
        Seed & 42 & seed fixed for reproducibility\\
        \hline
        Alpha ($\alpha$) & 0.9 & EMA smoothing factor for GFT \\
        \hline
        Beta ($\beta$) & 0.2 & GFT threshold factor \\
        \hline
        Warmup epochs & 3 (Transformers), variable (GraphMAE) & Initial epochs before applying early stopping \\
        \hline
        Patience & 5 & Patience for classic early stopping \\
        \hline
        Optimizer & Adam & Optimization algorithm \\
        \hline
        Learning rate & $10^{-4}$ & Step size for optimizer \\
        \hline
        Batch size & 32 & Number of samples per gradient update \\
        \hline
        Epochs & 30 (Transformers), 100 (GraphMAE) & Maximum number of training epochs \\
        \hline
    \end{tabular}
    \caption{Fine tuning hyperparameters and configuration.}
    \label{tab:hyperparameters}
\end{table}

\subsubsection{Stopping criteria}

The fine tuning process leverages two distinct stopping mechanisms depending on the chosen strategy.
\paragraph{Classic Strategy}
The baseline approach uses a traditional early stopping protocol based exclusively on predictive performance. This strategy monitors the validation metric and halts fine tuning if the model fails to improve upon the best recorded performance for a consecutive number of epochs specified by the \textit{patience} parameter. In this mode, energy consumption is tracked for comparison purposes but does not influence the stopping decision.

\paragraph{Green-Early Strategy (GFT)}
The proposed adaptive strategy dynamically evaluates the trade-off between performance gain and energy cost at each evaluation step. 
Instead of relying on a fixed patience, the algorithm computes the instantaneous efficiency ratio $GFT_t$, defined as the quotient between the relative improvement in validation performance ($\Delta P$) and the relative increase in cumulative emissions ($\Delta E$).
To filter out noise and volatility inherent in training metrics, this instantaneous ratio is smoothed using an EMA, denoted as $S_t$. 
The stopping decision is governed by a tolerance threshold $\beta$. The training halts when the current instantaneous efficiency ($GFT_t$) falls below a fraction $\beta$ of the historical smoothed trend ($S_t$):
\begin{equation*}
    GFT_t < \beta \cdot S_t
\end{equation*}


\subsubsection{Data Splitting}
The dataset was partitioned into fine tuning ($80\%$), validation ($10\%$), and test ($10\%$) sets using a scaffold splitting strategy.
Unlike random splitting, this method segregates molecules based on their two-dimensional structural frameworks (Bemis-Murcko scaffolds) computed using RDKit, thereby testing the model's ability to generalize to unseen chemical spaces.
Scaffolds were identified and sorted by cluster size in descending order to ensure that the most representative structures were prioritized for the training set.

To address the potential issue of class imbalanceâ€”where a scaffold-based split might result in validation or test sets lacking positive or negative examples a custom balancing algorithm was applied.
This procedure verifies the presence of all class labels in the target subsets.
If a subset (validation or test) contains fewer than two classes (in classification tasks), the algorithm searches the training set for scaffolds containing the missing label. To minimize the impact on dataset distribution, the algorithm prioritizes the smallest available scaffolds in the training set, moving them to the target set to restore class diversity while preserving structural separation. The training set is used to fine-tune the models, the validation set guides early stopping decisions, and the test set provides an unbiased evaluation of the final model performance.

\subsection{Metrics}
For classification tasks the ROC-AUC metric
\begin{equation}
    \text{ROC-AUC} = \frac{1}{2} \sum_{i=1}^{n} (X_i - X_{i-1})(Y_i + Y_{i-1})
\end{equation}
where $(X_i, Y_i)$ are the points on the ROC curve, is used for both validation and test evaluations. In this case higher values indicate better performance.
For regression tasks two different metrics were used:
\begin{itemize}
\item Relative Mean Squared Error (RMSE) metric
\begin{equation}
    \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\end{equation}
where $y_i$ are the true values and $\hat{y}_i$ are the predicted values, is used for validation evaluations.
\item Relative Squared Error (RSE) metric
\begin{equation}
    \text{RSE} = \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation}
where $y_i$ are the true values, $\hat{y}_i$ are the predicted values, and $\bar{y}$ is the mean of the true values, is used for test evaluations.
\end{itemize}
In both regression cases lower values indicate better performance.

\subsection{Results}
\subsubsection{BACE Dataset (Classification)}

\begin{table}[H]
    \tiny
    \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Model} & \textbf{AUROC (Classic)} & \textbf{AUROC (Early)} & \textbf{$\Delta$ Perf.} & \textbf{Ep. (C.)} & \textbf{Ep. (E.)} & \textbf{Emis. C.} & \textbf{Emis. E.} & \textbf{$\Delta$ Emiss.} \\
        \midrule
        ChemBERTa & 0.819 & 0.850 & \textcolor{blue}{+3.8\%} & 10 & 5 & $9.87 \times 10^{-4}$ & $5.68 \times 10^{-4}$ & \textcolor{blue}{-42.4\%} \\
        ChemBERTa-2 & 0.817 & 0.855 & \textcolor{blue}{+4.6\%} & 21 & 5 & $3.87 \times 10^{-4}$ & $2.70 \times 10^{-4}$ & \textcolor{blue}{-30.3\%} \\
        SELFormer & 0.819 & 0.832 & \textcolor{blue}{+1.6\%} & 8 & 6 & $1.52 \times 10^{-3}$ & $1.18 \times 10^{-3}$ & \textcolor{blue}{-22.4\%} \\
        SMILES-BERT & 0.789 & 0.844 & \textcolor{blue}{+7.0\%} & 15 & 5 & $2.71 \times 10^{-3}$ & $1.07 \times 10^{-3}$ & \textcolor{blue}{-60.5\%} \\
        GraphMAE (W10) & 0.680 & 0.667 & \textcolor{red}{-1.9\%} & 14 & 10 & $1.07 \times 10^{-4}$ & $7.18 \times 10^{-5}$ & \textcolor{blue}{-33.0\%} \\
        GraphMAE (W25) & 0.711 & 0.700 & \textcolor{red}{-1.5\%} & 29 & 25 & $1.91 \times 10^{-4}$ & $1.65 \times 10^{-4}$ & \textcolor{blue}{-13.4\%} \\
        GraphMAE (W50) & 0.758 & 0.748 & \textcolor{red}{-1.4\%} & 54 & 50 & $3.50 \times 10^{-4}$ & $4.17 \times 10^{-4}$ & \textcolor{red}{\textit{+19.0\%}} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison on Bace dataset}
\end{table}

\subsubsection{HIV Dataset (Classification)}
\begin{table}[H]
    \centering
    \tiny
    \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Model} & \textbf{AUROC (Classic)} & \textbf{AUROC (Early)} & \textbf{$\Delta$ Perf.} & \textbf{Ep. (C.)} & \textbf{Ep. (E.)} & \textbf{Emis. C.} & \textbf{Emis. E.} & \textbf{$\Delta$ Emiss.} \\
        \midrule
        ChemBERTa & 0.628 & 0.721 & \textcolor{blue}{+14.8\%} & 9 & 11 & $1.97 \times 10^{-2}$ & $2.40 \times 10^{-2}$ & \textcolor{red}{\textit{+22.1\%}} \\
        ChemBERTa-2 & 0.784 & 0.783 & \textcolor{red}{-0.1\%} & 14 & 5 & $4.31 \times 10^{-3}$ & $1.70 \times 10^{-3}$ & \textcolor{blue}{-60.6\%} \\
        SELFormer & 0.559 & 0.650 & \textcolor{blue}{+16.2\%} & 9 & 5 & $3.86 \times 10^{-2}$ & $2.16 \times 10^{-2}$ & \textcolor{blue}{-44.0\%} \\
        SMILES-BERT & 0.473 & 0.552 & \textcolor{blue}{+16.6\%} & 9 & 6 & $3.98 \times 10^{-2}$ & $2.66 \times 10^{-2}$ & \textcolor{blue}{-33.1\%} \\
        GraphMAE (W10) & 0.717 & 0.708 & \textcolor{red}{-1.2\%} & 14 & 10 & $1.66 \times 10^{-3}$ & $1.37 \times 10^{-3}$ & \textcolor{blue}{-17.7\%} \\
        GraphMAE (W25) & 0.749 & 0.741 & \textcolor{red}{-1.1\%} & 29 & 25 & $3.93 \times 10^{-3}$ & $3.43 \times 10^{-3}$ & \textcolor{blue}{-12.8\%} \\
        GraphMAE (W50) & 0.762 & 0.764 & \textcolor{blue}{+0.3\%} & 54 & 50 & $7.33 \times 10^{-3}$ & $6.24 \times 10^{-3}$ & \textcolor{blue}{-14.8\%} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison on Hiv dataset}
\end{table}


\subsubsection{BBBP Dataset (Classification)}
\begin{table}[H]
    \centering
    \tiny
    \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Model} & \textbf{AUROC (Classic)} & \textbf{AUROC (Early)} & \textbf{$\Delta$ Perf.} & \textbf{Ep. (C.)} & \textbf{Ep. (E.)} & \textbf{Emis. C.} & \textbf{Emis. E.} & \textbf{$\Delta$ Emiss.} \\
        \midrule
        ChemBERTa & 0.708 & 0.625 & \textcolor{red}{-11.8\%} & 9 & 5 & $3.58 \times 10^{-4}$ & $2.95 \times 10^{-4}$ & \textcolor{blue}{-17.5\%} \\
        ChemBERTa-2 & 0.771 & 0.667 & \textcolor{red}{-13.5\%} & 13 & 5 & $2.03 \times 10^{-4}$ & $2.11 \times 10^{-4}$ & \textcolor{red}{\textit{+4.2\%}} \\
        SELFormer & 0.604 & 0.646 & \textcolor{blue}{+6.9\%} & 9 & 5 & $3.52 \times 10^{-4}$ & $3.09 \times 10^{-4}$ & \textcolor{blue}{-12.1\%} \\
        SMILES-BERT & 0.542 & 0.438 & \textcolor{red}{-19.2\%} & 9 & 5 & $4.10 \times 10^{-4}$ & $3.07 \times 10^{-4}$ & \textcolor{blue}{-25.2\%} \\
        GraphMAE (W10) & 0.455 & 0.424 & \textcolor{red}{-6.7\%} & 14 & 10 & $9.72 \times 10^{-6}$ & $5.37 \times 10^{-6}$ & \textcolor{blue}{-44.7\%} \\
        GraphMAE (W25) & 0.485 & 0.485 & \textit{Unchanged} & 29 & 25 & $1.59 \times 10^{-5}$ & $1.18 \times 10^{-5}$ & \textcolor{blue}{-25.6\%} \\
        GraphMAE (W50) & 0.576 & 0.515 & \textcolor{red}{-10.5\%} & 54 & 50 & $2.75 \times 10^{-5}$ & $2.31 \times 10^{-5}$ & \textcolor{blue}{-16.0\%} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison on Bbbp dataset}
\end{table}



\subsubsection{CEP Dataset (Regression)}


\begin{table}[H]
    \centering
    \tiny
    \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Model} & \textbf{RSE (Classic)} & \textbf{RSE (Early)} & \textbf{$\Delta$ Perf.} & \textbf{Ep. (C.)} & \textbf{Ep. (E.)} & \textbf{Emis. C.} & \textbf{Emis. E.} & \textbf{$\Delta$ Emiss.} \\
        \midrule
        ChemBERTa & 0.306 & 0.289 & \textcolor{blue}{+5.5\%} & 12 & 7 & $1.93 \times 10^{-2}$ & $1.13 \times 10^{-2}$ & \textcolor{blue}{-41.2\%} \\
        ChemBERTa-2 & 0.337 & 0.280 & \textcolor{blue}{+16.9\%} & 18 & 5 & $3.52 \times 10^{-3}$ & $1.11 \times 10^{-3}$ & \textcolor{blue}{-68.6\%} \\
        SELFormer & 0.567 & 0.280 & \textcolor{blue}{+50.6\%} & 15 & 6 & $4.76 \times 10^{-2}$ & $1.92 \times 10^{-2}$ & \textcolor{blue}{-59.7\%} \\
        SMILES-BERT & 1.009 & 0.980 & \textcolor{blue}{+2.9\%} & 16 & 5 & $5.22 \times 10^{-2}$ & $1.65 \times 10^{-2}$ & \textcolor{blue}{-68.4\%} \\
        GraphMAE (W10) & 0.411 & 0.525 & \textcolor{red}{-27.8\%} & 100 & 10 & $9.64 \times 10^{-3}$ & $9.72 \times 10^{-4}$ & \textcolor{blue}{-89.9\%} \\
        GraphMAE (W25) & 0.423 & 0.481 & \textcolor{red}{-13.9\%} & 91 & 26 & $8.73 \times 10^{-3}$ & $2.51 \times 10^{-3}$ & \textcolor{blue}{-71.3\%} \\
        GraphMAE (W50) & 0.417 & 0.454 & \textcolor{red}{-8.8\%} & 99 & 50 & $9.53 \times 10^{-3}$ & $4.81 \times 10^{-3}$ & \textcolor{blue}{-49.6\%} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison on Cep dataset}
\end{table}


\subsubsection{Lipophilicity Dataset (Regression)}

\begin{table}[H]
    \centering
    \tiny
    \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Model} & \textbf{RSE (Classic)} & \textbf{RSE (Early)} & \textbf{$\Delta$ Perf.} & \textbf{Ep. (C.)} & \textbf{Ep. (E.)} & \textbf{Emis. C.} & \textbf{Emis. E.} & \textbf{$\Delta$ Emiss.} \\
        \midrule
        ChemBERTa & 0.525 & 0.601 & \textcolor{red}{-14.6\%} & 21 & 5 & $4.90 \times 10^{-3}$ & $1.34 \times 10^{-3}$ & \textcolor{blue}{-72.6\%} \\
        ChemBERTa-2 & 0.452 & 0.452 & \textcolor{red}{-0.1\%} & 23 & 5 & $8.20 \times 10^{-4}$ & $3.40 \times 10^{-4}$ & \textcolor{blue}{-58.5\%} \\
        SELFormer & 0.655 & 0.649 & \textcolor{blue}{+0.9\%} & 7 & 6 & $3.30 \times 10^{-3}$ & $2.85 \times 10^{-3}$ & \textcolor{blue}{-13.7\%} \\
        SMILES-BERT & 0.655 & 0.725 & \textcolor{red}{-10.7\%} & 10 & 5 & $4.75 \times 10^{-3}$ & $2.49 \times 10^{-3}$ & \textcolor{blue}{-47.6\%} \\
        GraphMAE (W10) & 0.608 & 0.968 & \textcolor{red}{-59.3\%} & 100 & 10 & $1.31 \times 10^{-3}$ & $1.36 \times 10^{-4}$ & \textcolor{blue}{-89.6\%} \\
        GraphMAE (W25) & 0.608 & 0.830 & \textcolor{red}{-36.6\%} & 100 & 25 & $1.31 \times 10^{-3}$ & $3.33 \times 10^{-4}$ & \textcolor{blue}{-74.7\%} \\
        GraphMAE (W50) & 0.608 & 0.685 & \textcolor{red}{-12.6\%} & 100 & 50 & $1.32 \times 10^{-3}$ & $6.53 \times 10^{-4}$ & \textcolor{blue}{-50.6\%} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison on Lipophilicity dataset}
\end{table}

\subsubsection{Malaria Dataset (Regression)}

\begin{table}[H]
    \centering
    \tiny
    \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Model} & \textbf{RSE (Classic)} & \textbf{RSE (Early)} & \textbf{$\Delta$ Perf.} & \textbf{Ep. (C.)} & \textbf{Ep. (E.)} & \textbf{Emis. C.} & \textbf{Emis. E.} & \textbf{$\Delta$ Emiss.} \\
        \midrule
        ChemBERTa & 1.058 & 1.149 & \textcolor{red}{-8.7\%} & 8 & 9 & $4.47 \times 10^{-3}$ & $5.00 \times 10^{-3}$ & \textcolor{red}{\textit{+11.7\%}} \\
        ChemBERTa-2 & 0.878 & 0.849 & \textcolor{blue}{+3.3\%} & 7 & 5 & $6.74 \times 10^{-4}$ & $5.15 \times 10^{-4}$ & \textcolor{blue}{-23.6\%} \\
        SELFormer & 1.164 & 1.096 & \textcolor{blue}{+5.9\%} & 10 & 11 & $1.07 \times 10^{-2}$ & $1.18 \times 10^{-2}$ & \textcolor{red}{\textit{+10.0\%}} \\
        SMILES-BERT & 0.991 & 0.991 & \textit{Unchanged} & 14 & 5 & $1.53 \times 10^{-2}$ & $5.61 \times 10^{-3}$ & \textcolor{blue}{-63.5\%} \\
        GraphMAE (W10) & 0.915 & 0.933 & \textcolor{red}{-2.0\%} & 16 & 10 & $8.80 \times 10^{-4}$ & $5.45 \times 10^{-4}$ & \textcolor{blue}{-38.1\%} \\
        GraphMAE (W25) & 0.980 & 0.953 & \textcolor{blue}{+2.7\%} & 29 & 25 & $1.66 \times 10^{-3}$ & $1.37 \times 10^{-3}$ & \textcolor{blue}{-17.8\%} \\
        GraphMAE (W50) & 1.104 & 1.057 & \textcolor{blue}{+4.3\%} & 54 & 50 & $2.96 \times 10^{-3}$ & $2.73 \times 10^{-3}$ & \textcolor{blue}{-7.9\%} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison on Malaria dataset}
\end{table}