\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{colortbl} 
\usepackage{adjustbox} 
\usepackage{emoji} % Per l'emoji del razzo

% Definizione colori custom
\definecolor{goodgreen}{RGB}{20, 150, 20}
\definecolor{alertred}{RGB}{200, 20, 20}

% Show a title slide at the start of each section
\AtBeginSection[]{
    \begin{frame}
        \centering
        \vfill
        {\huge \insertsectionhead}
        \vfill
    \end{frame}
}

\title{Green Fine Tuning for Molecular Property Prediction}
\author{Emanuele Fontana}
\date{}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Outline}
    \tableofcontents
\end{frame}

\section{Business Understanding}

\begin{frame}{Context: Green AI vs Red AI}
    % FIX: Ridotto larghezza colonne a 0.48 per evitare sovrapposizione
    \begin{columns}
        \column{0.45\textwidth}
        \begin{alertblock}{Red AI Issues}
            \begin{itemize}
                \item Maximizing accuracy regardless of cost.
                \item Massive models (Transformers, Large GNNs).
                \item \textbf{High CO$_2$ emissions \& Energy cost.}
            \end{itemize}
        \end{alertblock}

        \column{0.45\textwidth}
        \begin{exampleblock}{Green AI Solution}
            \begin{itemize}
                \item Efficiency as a core metric.
                \item Sustainable fine-tuning cycles.
                \item \textbf{UN Agenda 2030:} Goals 12 \& 13.
            \end{itemize}
        \end{exampleblock}
    \end{columns}
    \vspace{0.5cm}
    \centering
\end{frame}

\begin{frame}{Business Objectives}
    \textbf{Goal:} Integrate sustainable practices without losing predictive accuracy.
    
    \vspace{0.5cm}
    \begin{itemize}
        \item \textbf{Sustainability:} Reduce CO$_2$eq emissions by \textbf{20\% -- 50\%}.
        \item \textbf{Reliability:} Maintain performance within a \textbf{10\% margin} of error.
        \item \textbf{Trade-off Analysis:} Identify the trade-off between energy and accuracy.
    \end{itemize}
\end{frame}

\section{Data Understanding}

\begin{frame}{Dataset Description (MoleculeNet)}
    \begin{table}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{l l l r}
            \toprule
            \textbf{Dataset} & \textbf{Task} & \textbf{Key Features / Targets} & \textbf{Rows} \\
            \midrule
            \textbf{BACE} & Class. & \texttt{mol}, \texttt{Class} (Binary), \texttt{pIC50} & 1,513 \\
            \textbf{BBBP} & Class. & \texttt{smiles}, \texttt{p\_np} (Permeability) & 2,050 \\
            \textbf{HIV} & Class. & \texttt{smiles}, \texttt{HIV\_active} (0/1) & 41,126 \\
            \midrule
            \textbf{CEP} & Regr. & \texttt{smiles}, \texttt{PCE} (Efficiency) & 29,978 \\
            \textbf{Malaria} & Regr. & \texttt{smiles}, \texttt{activity} & 9,999 \\
            \textbf{Lipophilicity} & Regr. & \texttt{exp} (LogD), \texttt{smiles} & 4,200 \\
            \bottomrule
        \end{tabular}
        }
    \end{table}
\end{frame}

% RIPRISTINATA SLIDE SMILES
\begin{frame}{Data Representation: SMILES}
    \textbf{SMILES} (Simplified Molecular Input Line Entry System):
    \begin{itemize}
        \item ASCII strings representing chemical structures.
        \item \textbf{Atoms}: C, N, O... (Upper: aliphatic, Lower: aromatic).
        \item \textbf{Bonds}: Single (implicit), Double (=), Triple (\#).
        \item \textbf{Branching}: Parentheses ().
        \item \textbf{Rings}: Numbers (e.g., C1CCCCC1).
    \end{itemize}

\end{frame}

\section{Data Preparation}

\begin{frame}{Preprocessing Pipeline: RDKit}
    Pipeline  implemented:
    
    \vspace{0.3cm}
    \textbf{1. Parsing \& Validation}
    \begin{itemize}
        \item \texttt{Chem.MolFromSmiles}: Syntax check.
        \item \texttt{Chem.SanitizeMol}: Chemical consistency check.
        \begin{itemize}
            \item Valence Check, Kekulization, Aromaticity Detection.
        \end{itemize}
    \end{itemize}

    \vspace{0.3cm}
    \textbf{2. Output Formats}
    \begin{itemize}
        \item \textbf{Transformers:} Raw SMILES strings + Labels.
        \item \textbf{GNNs:} Graph Objects $G=(V,E)$ serialized as \texttt{.pt} files.
    \end{itemize}
\end{frame}

\begin{frame}{Graph Generation Details}
    Transformation of SMILES into Graph features for GraphMAE.
    
    \vspace{0.3cm}
    \begin{columns}
        \column{0.45\textwidth}
        \begin{block}{Node Features ($V$)}
            Captured for each atom:
            \begin{itemize}
                \item Atomic Number
                \item Chirality
                \item Degree
                \item Formal Charge
                \item Hybridization
                \item Aromaticity
                \item Num. Explicit Hs
            \end{itemize}
        \end{block}

        \column{0.45\textwidth}
        \begin{block}{Edge Features ($E$)}
            Captured for each bond:
            \begin{itemize}
                \item Bond Type (Single, Double, Triple, Aromatic)
                \item Stereochemistry
                \item Conjugation status
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\section{Modeling \& Strategy}

\begin{frame}{Architectures}
    \textbf{Sequence-based (Transformers):}
    \begin{itemize}
        \item \textbf{ChemBERTa} \& \textbf{ChemBERTa-2}: BERT-like models trained on chemical corpora.
        \item \textbf{SELFormer} \& \textbf{SMILES-BERT}: Specialized architectures.
    \end{itemize}
    
    \vspace{0.5cm}
    \textbf{Graph-based (GNNs):}
    \begin{itemize}
        \item \textbf{GraphMAE}: Masked Autoencoder. 
        \item Tested with variable warmup epochs (W10, W25, W50) to analyze stability.
    \end{itemize}
\end{frame}

\begin{frame}{Green Fine Tuning (GFT) Algorithm}
    \textbf{Objective:} Stop when marginal performance gain $<$ marginal energy cost.
    
    \begin{block}{Instantaneous Ratio}
        $$ GFT_t = \frac{\Delta P_t}{\Delta E_t} = \frac{Perf_t - Perf_{t-1}}{Emission_t - Emission_{t-1}} $$
    \end{block}
    
    \begin{block}{Smoothing \& Stopping Condition}
        We use Exponential Moving Average (EMA) with $\alpha=0.9$:
        $$ S_t = \alpha \cdot GFT_t + (1-\alpha) \cdot S_{t-1} $$
        \textbf{Stop Rule:} If $GFT_t < \beta \cdot S_t$ (with $\beta=0.2$).
    \end{block}
    
    \textit{Note: Energy tracked via CodeCarbon (Hardware: RTX 4070, Intel i7).}
\end{frame}

\section{Results: Trade-off Analysis}

\begin{frame}{BACE Dataset (Classification)}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{Results:}
        \begin{itemize}
            \item \textbf{SMILES-BERT:} +7.0\% Perf, -60.5\% Emissions.
            \item Early stopping effectively prevents overfitting.
        \end{itemize}
        
        \column{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Documentazione/img/tradeoff_experiments_bace.png}
    \end{columns}
\end{frame}

\begin{frame}{HIV Dataset (Classification)}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{Results:}
        \begin{itemize}
            \item \textbf{SELFormer:} +16.2\% Perf, -44.0\% Emissions.
            \item \textbf{ChemBERTa-2:} -60.6\% Emissions with stable performance.
        \end{itemize}
        
        \column{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Documentazione/img/tradeoff_experiments_hiv.png}
    \end{columns}
\end{frame}

\begin{frame}{BBBP Dataset (Classification)}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{Results:}
        \begin{itemize}
            \item Mixed results. Some models degrade (e.g., ChemBERTa).
            \item Highlighted importance of architecture selection.
        \end{itemize}
        
        \column{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Documentazione/img/tradeoff_experiments_bbbp.png}
    \end{columns}
\end{frame}

\begin{frame}{CEP Dataset (Regression)}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{Results:}
        \begin{itemize}
            \item \textbf{SELFormer:} Massive win (+50.6\% Perf, -59.7\% Emissions).
            \item GraphMAE shows strong trade-off sensitivity to warmup.
        \end{itemize}
        
        \column{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Documentazione/img/tradeoff_experiments_cep.png}
    \end{columns}
\end{frame}

\begin{frame}{Lipophilicity Dataset (Regression)}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{Challenge:}
        \begin{itemize}
            \item Most difficult dataset. 
            \item Significant trade-offs: GraphMAE W10 saved 89\% energy but lost 59\% accuracy.
            \item Requires longer training.
        \end{itemize}
        
        \column{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Documentazione/img/tradeoff_experiments_lipophilicity.png}
    \end{columns}
\end{frame}

\begin{frame}{Malaria Dataset (Regression)}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{Results:}
        \begin{itemize}
            \item \textbf{ChemBERTa-2:} +3.3\% Perf, -23.6\% Emissions.
            \item GraphMAE benefits significantly from longer warmup (W50).
        \end{itemize}
        
        \column{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Documentazione/img/tradeoff_experiments_malaria.png}
    \end{columns}
\end{frame}

\section{Explainability \& Conclusions}

\begin{frame}{What drives Emissions?}
    \textbf{Partial Correlation Analysis} ($\rho$):
    \vspace{0.3cm}
    
    \begin{enumerate}
        \item \textbf{Dataset Size ($\rho=0.65$):} The dominant factor. 
        \item \textbf{Epoch Efficiency ($\rho=0.26$):} Ratio of epochs used.
        \item \textbf{Model Architecture:} Secondary impact compared to data size.
    \end{enumerate}
    
    \vspace{0.3cm}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{../Documentazione/img/partial_correlation_importance.png}
    \end{center}
\end{frame}

\begin{frame}{Final Recommendations}
    \begin{block}{1. Adopt GFT}
        Achieved \textbf{60-80\% emission reduction} in best cases (Classification) often with performance gains.
    \end{block}

    \begin{block}{2. Data-Centric Approach}
        Since Dataset Size is the main emission driver, prioritize \textbf{Data Pruning} and Quality over Model Size.
    \end{block}

    \begin{block}{3. Adaptive Strategies}
        \begin{itemize}
            \item \textbf{Classification:} Transformers are highly robust to Early Stopping.
            \item \textbf{Regression/GNNs:} Require careful Warmup tuning (start with 50 epochs for complex tasks).
        \end{itemize}
    \end{block}
    
    \vspace{0.5cm}
    \centering
    % AGGIUNTO RAZZO
    \Large \textbf{Thank You! \emoji{rocket}}
\end{frame}

\end{document}